{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from torch.distributions import MultivariateNormal, Categorical\n",
    "import sys\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.append('../scripts/')\n",
    "from agents import GNN, Critic\n",
    "from environment import CustomEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: convert to script, write PPO class, push and pull to/from repo, move to HPC, change reward weights, Render to sanity check schedules produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "actor = GNN()\n",
    "critic = Critic()\n",
    "actor_optim = Adam(actor.parameters(), lr=lr)\n",
    "critic_optim = Adam(critic.parameters(), lr=lr)\n",
    "\n",
    "max_total_timesteps=10000\n",
    "max_timesteps_per_batch=1000\n",
    "k = 3  # num updates per batch\n",
    "gamma = 0.95 # discount factor\n",
    "clip = 0.2 # clip ratio min and max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init env\n",
    "env = CustomEnvironment() # default env\n",
    "env.set_params()\n",
    "env.timestep=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, _, _, _ = env.reset()\n",
    "# _, reward, _, _ = env.step(torch.argmax(actor(data)).item())\n",
    "# print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_log_prob(data):\n",
    "    edge_probs = actor(data)    # prob of each edge, Ex1\n",
    "    disp = Categorical(edge_probs)\n",
    "    action = disp.sample()\n",
    "    log_prob = disp.log_prob(action)\n",
    "    return action, log_prob # tensor, tensor w/ grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_value_log_prob(batch_data, batch_acts):\n",
    "    # NOTE: cannot apply actor in aggregate to batch_data because of HeteroData constraints\n",
    "    curr_log_probs = [Categorical(actor(data)).log_prob(batch_acts[i]) for i,data in enumerate(batch_data)]\n",
    "    curr_V = [critic(data) for data in batch_data]\n",
    "    # NOTE: do not detach\n",
    "    return torch.stack(curr_V), torch.stack(curr_log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rtgs(batch_rewards: list[list]):\n",
    "    batch_rtgs = []\n",
    "    for ep_rewards in reversed(batch_rewards):\n",
    "        discounted_reward = 0\n",
    "        for reward in reversed(ep_rewards):\n",
    "            discounted_reward = reward + discounted_reward * gamma\n",
    "            batch_rtgs.insert(0, discounted_reward)\n",
    "    return batch_rtgs # TODO: unsure why it becomes list of tensors instead of list of floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout():\n",
    "    batch_t = 0 # batch timesteps\n",
    "    batch_rewards, batch_data, batch_v, batch_acts, batch_log_probs = [],[],[],[],[]\n",
    "    while batch_t < max_timesteps_per_batch:\n",
    "        # start new episode\n",
    "        ep_reward = []\n",
    "        data, _, _, _ = env.reset()\n",
    "        for _ in range(env.max_timesteps):\n",
    "            batch_t +=1\n",
    "\n",
    "            # Append data\n",
    "            batch_data.append(data.clone()) # NOTE: really not ideal for memory\n",
    "\n",
    "            # Calculate estimated value\n",
    "            batch_v.append(critic(data))\n",
    "\n",
    "            # Calculate action, log_prob\n",
    "            action, log_prob = get_action_log_prob(data)\n",
    "            action = action.item()\n",
    "            log_prob = log_prob.detach().item()\n",
    "            batch_acts.append(action)\n",
    "            batch_log_probs.append(log_prob)\n",
    "\n",
    "            # Take step\n",
    "            data, reward, _, _ = env.step(action)\n",
    "\n",
    "            writer.add_scalar('Reward', reward)\n",
    "\n",
    "            # Collect step reward\n",
    "            ep_reward.append(reward)\n",
    "\n",
    "            if env.termination:\n",
    "                break\n",
    "        batch_rewards.append(ep_reward)\n",
    "\n",
    "    # At end of batch\n",
    "    # Calculate rewards-to-go\n",
    "    batch_rtgs = compute_rtgs(batch_rewards)\n",
    "\n",
    "    # Convert to tensors\n",
    "    batch_v = torch.stack(batch_v)                  # use stack to preserved grad_fn\n",
    "    batch_rtgs = torch.tensor(batch_rtgs)\n",
    "    batch_log_probs = torch.tensor(batch_log_probs)\n",
    "    batch_acts = torch.tensor(batch_acts)\n",
    "\n",
    "    return batch_data, batch_v, batch_acts, batch_log_probs, batch_rtgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_t = 0\n",
    "total_b = 0\n",
    "while total_t < max_total_timesteps:\n",
    "    # Create a new batch of length T = max_timesteps_per_batch\n",
    "    total_b += 1\n",
    "    print(\"Batch \", total_b, \" is \", total_t, \" timesteps in\")\n",
    "    batch_data, batch_v, batch_acts, batch_log_probs, batch_rtgs = rollout()\n",
    "    total_t += max_timesteps_per_batch\n",
    "\n",
    "    # Calculate advantage\n",
    "    A_k = (batch_rtgs - batch_v.detach()) # NOTE: does this detach V permanently?\n",
    "    A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "    \n",
    "    # Update k times per batch\n",
    "    for _ in range(k):\n",
    "        curr_V, curr_log_probs = get_current_value_log_prob(batch_data, batch_acts)\n",
    "\n",
    "        # Calculate ratios\n",
    "        ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "\n",
    "        # Calculate surrogate losses\n",
    "        surr1 = ratios * A_k\n",
    "        surr2 = torch.clamp(ratios, 1 - clip, 1 + clip)\n",
    "\n",
    "        actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "        critic_loss = nn.MSELoss()(curr_V, batch_rtgs)\n",
    "\n",
    "        actor_optim.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        actor_optim.step()\n",
    "\n",
    "        critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  1 , Episode  1  started at timestep  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/durdledoor/Library/Mobile Documents/com~apple~CloudDocs/2024_Projects/art-heist/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  1 , Episode  2  started at timestep  30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/durdledoor/Library/Mobile Documents/com~apple~CloudDocs/2024_Projects/art-heist/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  1 , Episode  3  started at timestep  60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/durdledoor/Library/Mobile Documents/com~apple~CloudDocs/2024_Projects/art-heist/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch  1 , Episode  4  started at timestep  90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/durdledoor/Library/Mobile Documents/com~apple~CloudDocs/2024_Projects/art-heist/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "total_t = 0\n",
    "total_e = 0\n",
    "total_b = 0\n",
    "while total_t < max_total_timesteps:\n",
    "    # Create a new batch of length T = max_timesteps_per_batch\n",
    "    total_b += 1\n",
    "    batch_t = 0\n",
    "    batch_rewards = []\n",
    "    batch_data, batch_v, batch_acts, batch_log_probs, batch_rtgs = [],[],[],[],[]\n",
    "    # hetero_data, tensor w/ grad, int, float, float\n",
    "    while batch_t < max_timesteps_per_batch and total_t < max_total_timesteps:\n",
    "        total_e += 1\n",
    "        print(\"Batch \", total_b, \", Episode \", total_e, \" started at timestep \", total_t)\n",
    "        ep_reward = []\n",
    "        data, reward, _, _ = env.reset()\n",
    "        for _ in range(env.max_timesteps):\n",
    "            batch_t +=1\n",
    "            total_t +=1\n",
    "\n",
    "            # Append data\n",
    "            batch_data.append(data.clone()) # NOTE: really not ideal for memory\n",
    "\n",
    "            # Calculate estimated value\n",
    "            batch_v.append(critic(data))\n",
    "\n",
    "            # Calculate action, log_prob\n",
    "            action, log_prob = get_action_log_prob(data)\n",
    "            action = action.item()\n",
    "            log_prob = log_prob.detach().item()\n",
    "            batch_acts.append(action)\n",
    "            batch_log_probs.append(log_prob)\n",
    "\n",
    "            # Take step\n",
    "            data, reward, _, _ = env.step(action)\n",
    "            \n",
    "            # Collect step reward\n",
    "            ep_reward.append(reward)\n",
    "\n",
    "            if env.termination:\n",
    "                data, reward, _, _ = env.reset()\n",
    "        batch_rewards.append(ep_reward)\n",
    "\n",
    "    # Calculate rewards-to-go\n",
    "    batch_rtgs = compute_rtgs(batch_rewards)\n",
    "\n",
    "    # Convert to tensors\n",
    "    batch_V = torch.stack(batch_v)                  # use stack to preserved grad_fn\n",
    "    batch_rtgs = torch.tensor(batch_rtgs)\n",
    "    batch_log_probs = torch.tensor(batch_log_probs)\n",
    "    batch_acts = torch.tensor(batch_acts)\n",
    "\n",
    "    # Calculate advantage\n",
    "    A_k = (batch_rtgs - batch_V.detach()) # NOTE: does this detach V permanently?\n",
    "    A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "    \n",
    "    # Update k times per batch\n",
    "    for _ in range(k):\n",
    "        curr_V, curr_log_probs = get_current_value_log_prob(batch_data, batch_acts)\n",
    "\n",
    "        # Calculate ratios\n",
    "        ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "\n",
    "        # Calculate surrogate losses\n",
    "        surr1 = ratios * A_k\n",
    "        surr2 = torch.clamp(ratios, 1 - clip, 1 + clip)\n",
    "\n",
    "        actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "        critic_loss = nn.MSELoss()(curr_V, batch_rtgs)\n",
    "\n",
    "        actor_optim.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        actor_optim.step()\n",
    "\n",
    "        critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1502103805541992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/durdledoor/Library/Mobile Documents/com~apple~CloudDocs/2024_Projects/art-heist/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "data, _, _, _ = env.reset()\n",
    "_, reward, _, _ = env.step(torch.argmax(actor(data)).item())\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PPO:\n",
    "#     def __init__(self, env):\n",
    "#         # Get environment information\n",
    "#         self.env = env\n",
    "#         self.thief_size = env.thief_size\n",
    "#         self.slot_size = env.slot_size\n",
    "#         self.heist_size = env.heist_size\n",
    "#         self._init_hyperparameters()\n",
    "\n",
    "#         # Init networks\n",
    "#         self.actor = GNN() # default channels\n",
    "#         self.critic = Critic()\n",
    "\n",
    "#         self.actor_optim = torch.optim.Adam(self.actor.parameters(), lr=0.001)\n",
    "#         self.critic_optim = torch.optim.Adam(self.critic.parameters(), lr=0.001)\n",
    "\n",
    "#     def learn(self, total_timesteps):\n",
    "#         t_so_far = 0\n",
    "#         while t_so_far < total_timesteps:\n",
    "#             batch_acts, batch_rtgs, batch_len = self.rollout()\n",
    "#             t_so_far += np.sum(batch_len)\n",
    "\n",
    "#             # Calculate V_{phi, k}\n",
    "#             V, _ = self.evaluate(batch_obs, batch_acts)\n",
    "\n",
    "#             # Calculate advantage\n",
    "#             A_k = batch_rtgs - V.detach()\n",
    "\n",
    "#             # Normalize advantages\n",
    "#             A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "\n",
    "#             for _ in range(self, self.n_updates_per_iteration):\n",
    "#                 # Calculate V_phi and pi_theta(a_t | s_t)\n",
    "#                 _, curr_log_probs = self.evaluate(batch_obs, batch_acts)\n",
    "\n",
    "#                 # Calculate ratios\n",
    "#                 ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "\n",
    "#                 # Calculate surrogate losses\n",
    "#                 surr1 = ratios * A_k\n",
    "#                 surr2 = torch.clamp(ratios, 1 - self.clip, 1 + self.clip)\n",
    "\n",
    "#                 actor_loss = (-torch.min(surr1, surr2)).mean()\n",
    "#                 critic_loss = nn.MSELoss()(V, batch_rtgs)\n",
    "\n",
    "#                 self.actor_optim.zero_grad()\n",
    "#                 actor_loss.backward(retain_graph=True)\n",
    "#                 self.actor_optim.step()\n",
    "\n",
    "#                 self.critic_optim.zero_grad()\n",
    "#                 critic_loss.backward()\n",
    "#                 self.critic_optim.step()\n",
    "\n",
    "#     def rollout(self):\n",
    "#         # Batch data\n",
    "#         # batch_obs = []          # batch data, too big to store\n",
    "#         batch_acts = []         # batch actions\n",
    "#         # batch_log_probs = []    # batch log_probs, too big to store\n",
    "#         batch_rewards = []      # batch reward\n",
    "#         batch_rtgs = []         # batch reward to gos\n",
    "#         batch_lens = []         # batch episode lengths\n",
    "\n",
    "#         t= 0\n",
    "#         while t <  self.timesteps_per_batch:\n",
    "#             ep_rewards = []\n",
    "#             data, reward, _, _ = self.env.reset()\n",
    "#             done=False\n",
    "#             while not env.termination:\n",
    "#                 # Increment timesteps ran this batch so far\n",
    "#                 t += 1\n",
    "#                 # Collect observation\n",
    "#                 action, log_prob = self.get_action(data)\n",
    "#                 next_data, reward, _, _ = self.env.step(action)\n",
    "\n",
    "#                 # Collect reward, action, and log_prob\n",
    "#                 ep_rewards.append(reward)\n",
    "#                 batch_acts.append(action)\n",
    "\n",
    "#                 if done: break\n",
    "            \n",
    "#             # Collect episodic length and reward\n",
    "#             batch_lens.append(self.env.timestep)\n",
    "#             batch_rewards.append(ep_rewards)\n",
    "        \n",
    "#         # Return batch data\n",
    "#         batch_rtgs = self.compute_rtgs(batch_rewards)\n",
    "#         return batch_acts, batch_rtgs, batch_lens\n",
    "    \n",
    "#     def get_action(self, data):\n",
    "#         edge_probs = self.actor(data)\n",
    "\n",
    "#         # sample action # NOTE: hard b/c E changes every step\n",
    "#         dist = Categorical(edge_probs)\n",
    "#         action = dist.sample()\n",
    "#         log_prob = dist.log_prob(action)\n",
    "#         return action, log_prob.detach().numpy()\n",
    "    \n",
    "#     def compute_rtgs(self, batch_rewards):\n",
    "#         # shape: num timesteps per episode\n",
    "#         batch_rtgs = []\n",
    "#         for ep_rewards in reversed(batch_rewards):\n",
    "#             discounted_reward = 0\n",
    "#             for reward in reversed(ep_rewards):\n",
    "#                 discounted_reward = reward + discounted_reward * self.gamma\n",
    "#                 batch_rtgs.insert(0, discounted_reward)\n",
    "#         return batch_rtgs\n",
    "    \n",
    "#     def _init_hyperparameters(self):\n",
    "#         # default hyperparameters\n",
    "#         self.gamma = 0.95\n",
    "#         self.timesteps_per_batch = 5\n",
    "\n",
    "#     def evaluate(self, batch_obs, batch_acts):\n",
    "#         V = self.critic(batch_obs).squeeze()\n",
    "#         dist = Categorical(self.actor(batch_obs))\n",
    "#         action = dist.sample()\n",
    "#         log_probs = dist.log_prob(action)\n",
    "#         return V, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/durdledoor/Library/Mobile Documents/com~apple~CloudDocs/2024_Projects/art-heist/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "for i in range(5):\n",
    "    action = torch.argmax(actor(data)).item()\n",
    "    data, _, _, _ = env.step(action)\n",
    "    test.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, _, _, _ = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/durdledoor/Library/Mobile Documents/com~apple~CloudDocs/2024_Projects/art-heist/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "edge_probs = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cov = torch.diag(torch.full(size=(len(edge_probs),), fill_value=0.5))\n",
    "# dist = MultivariateNormal(edge_probs, cov)\n",
    "# action_probs = dist.sample()\n",
    "# log_probs = dist.log_prob(action_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Pendulum-v0')\n",
    "model = PPO(env)\n",
    "model.learn(10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
